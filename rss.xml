<?xml version="1.0" encoding="UTF-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Gallnut</title><description>Welcome</description><link>https://gallnut.github.io/</link><language>zh_CN</language><item><title>PAC 学习框架</title><link>https://gallnut.github.io/posts/pac_learning/</link><guid isPermaLink="true">https://gallnut.github.io/posts/pac_learning/</guid><pubDate>Sun, 18 Jan 2026 00:00:00 GMT</pubDate><content:encoded>&lt;h1&gt;PAC 学习模型&lt;/h1&gt;
&lt;p&gt;基本的符号说明为，$\mathcal{X}$ 代表样本或者样本空间，$\mathcal{Y}$ 代表标签或者目标空间，简单起见，假设 $\mathcal{Y} = {0, 1}$ 对应于二分类问题，其他场景的理论分析只需简单推广。$c$ 代表一个概念，定义为 $c := \mathcal{X} \rightarrow \mathcal{Y}$ 。概念类代表所有可能概念的集合，记为 $\mathcal{C}$ 。&lt;/p&gt;
&lt;p&gt;假设样本来源于某个固定但未知的分布 $\mathcal{D}$，样本间相互独立。学习者考虑一组固定的可能的概念 $\mathcal{H}$ 称为假设集，与 $\mathcal{C}$ 不一定一致。学习者接受一个来源于 $\mathcal{D}$ 的独立同分布的样本 $S = (x_1, \cdots, x_m)$ 以及对应的标签 $(c(x_1), \cdots, c(x_m))$, $c \in \mathcal{C}$ 是一个要学习的目标概念。学习任务就是，利用这些有标签的数据样本 $S$，去选择出一个假设 $h_S \in \mathcal{H}$ 使得该假设针对目标概念 $c$ 达到最小的泛化误差 $R(h)$。&lt;/p&gt;
&lt;h2&gt;泛化误差&lt;/h2&gt;
&lt;p&gt;给定一个假设 $h \in \mathcal{H}$, 一个目标概念 $c \in \mathcal{C}$，和一个底层分布 $\mathcal{D}$， $h$ 的泛化误差或者说 risk 定义如下：
$$
\begin{equation*}
R(h) = \mathbb{P}&lt;em&gt;{x \sim \mathcal{D}}[h(x) \neq c(x)] = \mathbb{E}&lt;/em&gt;{x \sim \mathcal{D}}[\mathbb{1}_{h(x) \neq c(x)}]
\end{equation*}
$$
泛化误差无法直接获取，因为底层分布和目标概念都是未知的。但学习者可以在样本上计算经验误差。&lt;/p&gt;
&lt;h2&gt;经验误差&lt;/h2&gt;
&lt;p&gt;给定一个假设 $h \in \mathcal{H}$, 一个目标概念 $c \in \mathcal{C}$，和一个样本 $S = (x_1, \cdots, x_m)$，经验误差或者经验 risk 定义如下：
$$
\begin{equation*}
\widehat{R}&lt;em&gt;S(h)=\frac{1}{m}\sum&lt;/em&gt;{i=1}^{m}\mathbb{1}&lt;em&gt;{h(x_i) \neq c(x_i)}
\end{equation*}
$$
由于样本是独立同分布的，所以有：
$$
\begin{equation*}
\mathbb{E}&lt;/em&gt;{S \sim \mathcal{D}^m}[\widehat{R}_S(h)] = R(h)
\end{equation*}
$$&lt;/p&gt;
&lt;h2&gt;PAC 学习&lt;/h2&gt;
&lt;p&gt;一个概念类 $\mathcal{C}$ 被认为是 PAC 可学习的，当存在一个算法 $\mathcal{A}$ 和一个多项式函数 $poly(\cdot,\cdot,\cdot,\cdot)$ 使得对任意的 $\epsilon \gt 0$ 和 $\delta \gt 0$, 对所有的分布 $\mathcal{D}$ 和目标概念 $c \in \mathcal{C}$，当样本规模 $m \ge poly(\frac{1}{\epsilon},\frac{1}{\delta},n,size(c))$ 时，有如下式子成立：
$$
\begin{equation*}
\mathbb{P}_{S \sim \mathcal{D}^m}[R(h_S) \le \epsilon] \ge 1 - \delta
\end{equation*}
$$
如果 $\mathcal{A}$ 运行在 $poly(\frac{1}{\epsilon},\frac{1}{\delta},n,size(c))$ ，则被认为是概念类的一种高效 PAC 可学习算法。&lt;/p&gt;
&lt;h3&gt;有限假设集一致性场景&lt;/h3&gt;
&lt;p&gt;$\mathcal{H}$ 是一个将 $\mathcal{X}$ 映射到 $\mathcal{Y}$ 的函数的有限集合，一致性假设的定义是，对于任意的目标概念 $c$ 和独立同分布的样本 $S$, $h_S := \widehat{R}&lt;em&gt;S(h_S) = 0$。对于这样的情形有，对于任意的 $\epsilon \gt 0$ 和 $\delta \gt 0$, 当 $m \ge \frac{1}{\epsilon}(\log{|\mathcal{H}|} + \log{\frac{1}{\delta}})$ 时，有如下式子成立：
$$
\begin{equation*}
\mathbb{P}&lt;/em&gt;{S \sim \mathcal{D}^m}[R(h_S) \le \epsilon] \ge 1 - \delta
\end{equation*}
$$
这实际上定义了一个泛化误差边界，对任意的 $\epsilon \gt 0$ 和 $\delta \gt 0$,在至少 $1 - \delta$ 的概率下有
$$
\begin{equation*}
R(h_S) \le \frac{1}{m}(\log{|\mathcal{H}|} + \log{\frac{1}{\delta}})
\end{equation*}
$$&lt;/p&gt;
&lt;h3&gt;有限假设集非一致性场景&lt;/h3&gt;
&lt;p&gt;首先，对于任意的假设 $h: X \rightarrow {0, 1}$ 有下面的不等式成立：
$$
\mathbb{P}_{S \sim \mathcal{D}^m}[\widehat{R}_S(h) - R(h) \ge \epsilon] \le \exp(-2m\epsilon^2)
$$&lt;/p&gt;
&lt;p&gt;$$
\mathbb{P}_{S \sim \mathcal{D}^m}[\widehat{R}&lt;em&gt;S(h) - R(h) \le -\epsilon] \le \exp(-2m\epsilon^2)
$$
进一步可以得到如下不等式：
$$
\begin{equation*}
\mathbb{P}&lt;/em&gt;{S \sim \mathcal{D}^m}[|\widehat{R}_S(h) - R(h)| \ge \epsilon] \le 2\exp(-2m\epsilon^2)
\end{equation*}
$$
证明利用了霍夫丁不等式。
由上述不等式可得，对于任意一个确定的假设 $h: \mathcal{X} \rightarrow {0, 1}$, 对任意的 $\delta \gt 0$， 在至少 $1 - \delta$ 的概率下，有：
$$
\begin{equation*}
R(h) \le \widehat{R}_S(h) + \sqrt{\frac{\log{\frac{2}{\delta}}}{2m}}
\end{equation*}
$$
进一步推广，对于一个有限的假设集 $\mathcal{H}$，对任意的 $\delta \gt 0$ ，有至少 $1 - \delta$ 的概率，如下不等式成立：
$$
\begin{equation*}
\forall h \in \mathcal{H}, \quad R(h) \le \widehat{R}_S(h) + \sqrt{\frac{\log{|\mathcal{H}|} + \log{\frac{2}{\delta}}}{2m}}
\end{equation*}
$$
证明如下：
$$
\begin{align}
\mathbb{P}[\exists h \in \mathcal{H} |\widehat{R}&lt;em&gt;S(h) &amp;amp;- R(h)| \gt \epsilon] \
&amp;amp;= \mathbb{P}[(|\widehat{R}&lt;em&gt;S(h_1) - R(h_1)| \gt \epsilon) \lor \cdots \lor (|\widehat{R}&lt;em&gt;S(h&lt;/em&gt;{|\mathcal{H}|}) - R(h&lt;/em&gt;{|\mathcal{H}|})| \gt \epsilon)] \
&amp;amp;\le \sum&lt;/em&gt;{h \in \mathcal{H}}\mathbb{P}[|\widehat{R}_S(h) - R(h)|] \
&amp;amp;\le 2|\mathcal{H}|\exp(-2m\epsilon^2)
\end{align}
$$&lt;/p&gt;
&lt;h2&gt;不可知 PAC 学习&lt;/h2&gt;
&lt;p&gt;所谓不可知，即函数对于输入到标签的映射不是确定的，是一个概率函数。
如果存在一个多项式 $poly(\cdot,\cdot,\cdot,\cdot)$ 使得对于任意的 $\epsilon \gt 0$ 和 $\delta \gt 0$, 对于 $\mathcal{X} \times \mathcal{Y}$ 上的所有分布 $\mathcal{D}$，当样本规模 $m \ge poly(\frac{1}{\epsilon},\frac{1}{\delta},n,size(c))$ 时，有如下式子成立：
$$
\mathbb{P}&lt;em&gt;{S \sim \mathcal{D}^m}[R(h_S) - \min&lt;/em&gt;{h \in \mathcal{H}} \le \epsilon] \ge 1 - \delta
$$
如果进一步算法在 $poly$ 下运行，则认为是一个高效的不可知PAC学习算法。&lt;/p&gt;
&lt;h2&gt;贝叶斯误差&lt;/h2&gt;
&lt;p&gt;对于一个 $\mathcal{X} \times \mathcal{Y}$ 上的分布 $\mathcal{D}$，贝叶斯误差 $R^&lt;em&gt;$ 可测函数 $h: \mathcal{X} \rightarrow \mathcal{Y}$ 泛化误差的下确界：
$$
\begin{equation&lt;/em&gt;}
R^* = \inf_{h} R(h)
\end{equation*}
$$
使得泛化误差等于贝叶斯误差的那个假设 $h$ 被称为贝叶斯假设或者贝叶斯分类器。&lt;/p&gt;
&lt;h2&gt;拉德马赫复杂度&lt;/h2&gt;
&lt;p&gt;拉德马赫复杂度用于定义假设集合的复杂程度。首先定义函数族 $\mathcal{G}$
$$
\begin{equation*}
\mathcal{G} = {g: (x, y) \rightarrow L(h(x), y): h \in \mathcal{H}}
\end{equation*}
$$
其中$L$是损失函数。&lt;/p&gt;
&lt;h3&gt;经验拉德马赫复杂度&lt;/h3&gt;
&lt;p&gt;$\mathcal{Z} = \mathcal{X} \times \mathcal{Y}$ , $\mathcal{G}$ 是从 $\mathcal{Z}$ 映射到 $[a, b]$ 的函数族，对于给定的样本 $S = (z_1, \cdots, z_m)$, $\mathcal{G}$ 的经验拉德马赫复杂度定义如下：
$$
\begin{equation*}
\widehat{\mathfrak{R}}&lt;em&gt;S(\mathcal{G}) = \mathbb{E}&lt;/em&gt;{\sigma}[\sup_{g \in \mathcal{G}}\frac{1}{m}\sum_{i = 1}^{m}\sigma_i g(z_i)]
\end{equation*}
$$
其中 $\mathbf{\sigma} = (\sigma_1, \cdots, \sigma_m)^{\top}$, $\sigma_i$ 是独立的均匀分布随机变量，取值于 ${-1, +1}$。该随机变量被称为拉德马赫变量。
代表了假设函数族与随机变量（噪声）的拟合程度，越是丰富的函数族，越是有能力拟合随机噪声。
$$
\begin{equation*}
\widehat{\mathfrak{R}}&lt;em&gt;S(\mathcal{G}) = \mathbb{E}&lt;/em&gt;\sigma[\sup_{g \in \mathcal{G}}\frac{\sigma \cdot \mathsf{g}_S}{m}]
\end{equation*}
$$
设 $\mathcal{D}$ 是样本来源的分布，拉德马赫复杂度定义为经验拉德马赫复杂度的期望：
$$
\begin{equation*}
\mathfrak{R}&lt;em&gt;m(\mathcal{G}) = \mathbb{E}&lt;/em&gt;{S \sim \mathcal{D}^m}[\widehat{\mathfrak{R}}_S(\mathcal{G})]
\end{equation*}
$$&lt;/p&gt;
&lt;h2&gt;估计误差与逼近误差&lt;/h2&gt;
&lt;h3&gt;超额误差&lt;/h3&gt;
&lt;p&gt;$h \in \mathcal{H}$ 的超额误差定义为泛化误差 $R(h)$ 与贝叶斯误差 $R^&lt;em&gt;$ 的差值：
$$
\begin{equation&lt;/em&gt;}
R_{excess}(h) = R(h) - R^*
\end{equation*}
$$
超额误差可以作如下分解：
$$
\begin{equation*}
R(h) - R^* = \underbrace{(R(h) - \inf_{h \in \mathcal{H}}R(h))}&lt;em&gt;{estimation} + \underbrace{(\inf&lt;/em&gt;{h \in \mathcal{H}}R(h) - R^&lt;em&gt;)}_{approximation}
\end{equation&lt;/em&gt;}
$$
&lt;img src=&quot;./estimation_approximation.png&quot; alt=&quot;&quot; /&gt;
第一项称为估计误差，第二项称为逼近误差。估计误差取决于所选假设 $h$，它是衡量 $h$ 相对于 $\mathcal{H}$ 中假设所产生误差的下确界之间的误差，表示与当前假设类中最优假设的差距。逼近误差，衡量在假设类中的最优假设与全局最优假设贝叶斯分类器之间的差距，这是假设类 $\mathcal{H}$ 的性质。&lt;/p&gt;
</content:encoded></item><item><title>U-统计量和V-统计量</title><link>https://gallnut.github.io/posts/statistic/</link><guid isPermaLink="true">https://gallnut.github.io/posts/statistic/</guid><pubDate>Sat, 03 Jan 2026 00:00:00 GMT</pubDate><content:encoded>&lt;h1&gt;U-统计量&lt;/h1&gt;
&lt;h2&gt;定义&lt;/h2&gt;
&lt;p&gt;定义 $h(x_1, \cdots, x_r): \mathbb{R}^r \rightarrow \mathbb{R}$ 为一个函数，其具有对称性，即交换任意 $x_i$, $x &lt;em&gt;j$ 的位置， $h$ 的值保持不变。对随机变量 $X_1, \cdots, X_n$, 基于 $h$ 的 U-统计量定义如下：
$$
\begin{equation*}
U_n = \frac{1}{n \choose r} \sum&lt;/em&gt;{1 \le i_1 \lt \cdots \lt i_r \le n}h(X_{i_1}, \cdots, X_{i_r})
\end{equation*}
$$
这里， $h(\cdot)$ 称为U-统计量的核函数（Kernel function）， 而核函数的维数 $r$ 称为该U-统计量的度（degree）。&lt;/p&gt;
&lt;h2&gt;两样本U-统计量&lt;/h2&gt;
&lt;p&gt;定义 $h(x_1, \cdots, x_r; y_1, \cdots, y_s): \mathbb{R}^{r+s} \rightarrow \mathbb{R}$ 为一个函数，其对 $X$ 和 $Y$ 分别具有对称性，即交换任意 $x_{i_1}$， $x_{i_2}$ 的位置或交换任意 $y_{j_1}$ 和 $y_{j_2}$ 的位置， $h$ 的值保持不变（但不能随意交换 $x_i$ 和 $y_j$）。对随机变量 $X_1, \cdots, X_m; Y_1, \cdots, Y_n$ ，基于 $h$ 的两样本U-统计量定义如下：
$$
\begin{equation*}
U_{m, n} = \frac{1}{{m \choose r}{n \choose s}}\sum_{1 \le i_1 \lt \cdots \lt i_r \le m} \sum_{1 \le j_1 \lt \cdots \lt j_s \le n} h(X_1, \cdots, X_m; Y_1, \cdots, Y_n)
\end{equation*}
$$
目前在机器学习中，最常见的情形是 $r = s = 1$ ，例如能量距离和最大平均差异。&lt;/p&gt;
&lt;h1&gt;Hoeffding的ANOVA分解定理&lt;/h1&gt;
&lt;h2&gt;定理表述&lt;/h2&gt;
&lt;p&gt;Hoeffding的ANOVA分解定理是现代U-统计量理论的基础。为表述该定理， 定义：$\mu = \mathbb{E}[h(X_1, \cdots, X_r)]$。对所有的 $1 \le k \le r$ ，定义投影函数：
$$
\begin{equation*}
a_k(x_1, \cdots, x_k) = \mathbb{E}[h(X_1, \cdots, X_r) | X_1 = x_1, \cdots, X_k = x_k] - \mu
\end{equation*}
$$
然后定义正交化投影函数：
$g_1(x_1) = a_1(x_1)$， $g_2(x_1, x_2) = a_2(x_1, x_2) - g_1(x_1) - g_1(x_2)$， 等等。每一个 $g_k$ 都定义为相应的 $a_k$ 减去之前定义过的所有 $g_1, \cdots, g_{k-1}$， 直至最后一个函数 $g_r$：
$$
\begin{equation*}
g_r(x_1, \cdots, x_r) = a_r(x_1, \cdots, x_r) - \sum_{j=1}^{r-1}\sum_{1 \le i_1 \lt \cdots \lt i_j \le r}g_j(x_{i_1}, \cdots, x_{i_j})
\end{equation*}
$$
Hoeffding的ANOVA分解定理的内容是：
$$
\begin{equation*}
U_n - \mu = {n \choose r}^{-1}\sum_{k=1}^{r}{n - k \choose r - k}\cdot\sum_{1 \le i_1 \lt \cdots \lt i_k \le n}g_k(X_{i_1}, \cdots, X_{i_k})
\end{equation*}
$$
或者如下形式：
$$
\begin{equation*}
U_n - \mu = \sum_{k=1}^{r}{r \choose k}{n \choose k}^{-1}\cdot\sum_{1 \le i_1 \lt \cdots \lt i_k \le n}g_k(X_{i_1}, \cdots, X_{i_k})
\end{equation*}
$$&lt;/p&gt;
&lt;h2&gt;分解项的性质&lt;/h2&gt;
&lt;p&gt;所有正交化投影函数 $g_k$ 都满足：
$$
\begin{equation*}
\mathbb{E}[g_k(X_1, \cdots, X_k) | X_1, \cdots, X_k] = 0
\end{equation*}
$$
因此，所有的分解项之间是互不相关的，并且度为 $k$ 的分解项之平均的阶为 $O_p(n ^ {- k / 2})$ 。
在大多数应用中，一个U-统计量的ANOVA分解中最重要的是前一项或前两项。根据分解项的性质，可以得到如下的两项ANOVA分解式：
$$
\begin{equation*}
U_n - \mu = \frac{r}{n}\sum_{i=1}^{n}g_1(X_i) + \frac{r(r-1)}{n(n-1)}\sum_{1 \le i \lt j \le n}g_2(X_i, X_j) + O_p(n ^ {- 3 / 2})
\end{equation*}
$$&lt;/p&gt;
&lt;h1&gt;V-统计量&lt;/h1&gt;
&lt;p&gt;V-统计量与U-统计量形式相似，且统计性质上有紧密联系。每个V-统计量对应一个U-统计量，很多情况下，V-统计量的渐进分布，只是相应的U-统计量的渐进分布经过一些修饰的版本。&lt;/p&gt;
&lt;h2&gt;定义&lt;/h2&gt;
&lt;p&gt;定义 $h(x_1, \cdots, x_r): \mathbb{R}^{r} \rightarrow \mathbb{R}$， 其具有对称性，即交换任意 $x_i$， $x_j$ 的位置，$h$ 的值保持不变。对随机变量 $X_1, \cdots, X_n$， 基于 $h$ 的V-统计量定义如下：
$$
\begin{equation*}
V_n = \frac{1}{n^r}\sum_{1 \le {i_1, \cdots, i_r} \le n}h(X_{i_1}, \cdots, X_{i_r})
\end{equation*}
$$
这里 $h(\cdot)$ 称为V-统计量的核函数（Kernel function），而核函数的维数 $r$ 称为该V-统计量的度（degree）。&lt;/p&gt;
&lt;h2&gt;与U-统计量的区别&lt;/h2&gt;
&lt;p&gt;上述定义中，求和下标 $i_1, \cdots, i_r$ 互相之间不必两两相同，这是V-统计量与U-统计量唯一的区别。&lt;/p&gt;
&lt;h2&gt;性质&lt;/h2&gt;
&lt;p&gt;V-统计量和U-统计量是渐进等价的，也就是说，当 $n$ 较大时，应用中一般不区分两者。
V-统计量用作参数估计时，一般是有偏的。但因其和U-统计量的渐进等价性，这种偏差在大样本情形下可以忽略。&lt;/p&gt;
&lt;h2&gt;与U-统计量的关系&lt;/h2&gt;
&lt;p&gt;$$
\begin{equation*}
V_n = U_n + O_p(n^{-1})
\end{equation*}
$$
更精确的有：
$$
\begin{equation*}
V_n = U_n + \sum_{k=1}^{r-1}\frac{c_k}{n^k}\sum g_k(X_{i_1}, \cdots, X_{i_k})
\end{equation*}
$$
$$
\begin{equation*}
c_k = \frac{r \choose k}{k!}\sum_{\pi \in \mathcal{P}&lt;em&gt;{r,k}}\prod&lt;/em&gt;{j=1}^{k}|\pi_{j}|!
\end{equation*}
$$
其中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\mathcal{P}_{r, k}$：把 $r$ 个位置划分成 $k$ 个非空块的所有划分&lt;/li&gt;
&lt;li&gt;$|\pi_j|$：第 $j$ 个块的大小&lt;/li&gt;
&lt;/ul&gt;
</content:encoded></item><item><title>shared_ptr 实现</title><link>https://gallnut.github.io/posts/cc_shared_ptr/</link><guid isPermaLink="true">https://gallnut.github.io/posts/cc_shared_ptr/</guid><pubDate>Wed, 05 Mar 2025 00:00:00 GMT</pubDate><content:encoded>&lt;h1&gt;shared_ptr 实现&lt;/h1&gt;
&lt;p&gt;标准库的std::shared_ptr&amp;lt;?&amp;gt;模板支持定义资源的构造器和释放器，这里只进行简单实现，不涉及复杂的模板技法。&lt;/p&gt;
&lt;h2&gt;模板类定义&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;#include &amp;lt;atomic&amp;gt;
#include &amp;lt;cstddef&amp;gt;

template &amp;lt;typename T&amp;gt;
class shared_ptr
{
public:
    shared_ptr();
    explicit shared_ptr(T* ptr);

    ~shared_ptr();

    shared_ptr(const shared_ptr&amp;lt;T&amp;gt;&amp;amp; other);
    shared_ptr&amp;lt;T&amp;gt;&amp;amp; operator=(std::nullptr_t);
    shared_ptr&amp;lt;T&amp;gt;&amp;amp; operator=(const shared_ptr&amp;lt;T&amp;gt;&amp;amp; other);

    shared_ptr(shared_ptr&amp;lt;T&amp;gt;&amp;amp;&amp;amp; other) noexcept;
    shared_ptr&amp;lt;T&amp;gt;&amp;amp; operator=(shared_ptr&amp;lt;T&amp;gt;&amp;amp;&amp;amp; other) noexcept;

    T&amp;amp; operator*() const;

    std::size_t use_count() const;

    bool unique() const;
    
    T* get() const;
    
    void reset(T* ptr = nullptr);

private:
    void release();

private:
    T* ptr_;
    std::atomic&amp;lt;std::size_t&amp;gt;* ref_cnt_;
};
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;使用泛型指针表示该智能指针维护的资源，引用计数使用指针，可以在多个智能指针对象之间共享，使用原子变量保证引用计数操作的线程安全。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;使用 std::atomic&amp;lt;std::size_t&amp;gt; * 而不是 std::atomic&amp;lt;std::size_t *&amp;gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;release 实现&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;template &amp;lt;typename T&amp;gt;
void shared_ptr&amp;lt;T&amp;gt;::release()
{
    if (ref_cnt_ &amp;amp;&amp;amp; ref_cnt_-&amp;gt;fetch_sub(1, std::memory_order_release) == 1)
    {
        std::atomic_thread_fence(std::memory_order_acquire);
        delete ptr_;
        delete ref_cnt_;
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ref_cnt-&amp;gt;fetch_sub(1, std::memory_order_release) 使用release内存序，保证引用计数修改之前的操作对其他线程可见。只有最后一个线程在释放资源时需要对其他线程的操作可见，所以只有fetch_sub返回1时才需要设置acquire屏障，避免直接对fetch_sub设置acq_rel屏障。&lt;/p&gt;
&lt;h2&gt;普通构造与析构&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;template &amp;lt;typename T&amp;gt;
shared_ptr&amp;lt;T&amp;gt;::shared_ptr()
    : ptr_(nullptr)
    , ref_cnt_(nullptr)
{}

template &amp;lt;typename T&amp;gt;
shared_ptr&amp;lt;T&amp;gt;::shared_ptr(T* ptr)
    : ptr_(ptr)
    , ref_cnt_(ptr ? new std::atomic&amp;lt;std::size_t&amp;gt;(1) : nullptr)
{}

template &amp;lt;typename T&amp;gt;
shared_ptr&amp;lt;T&amp;gt;::~shared_ptr()
{
    release();
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;拷贝构造与拷贝赋值&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;template &amp;lt;typename T&amp;gt;
shared_ptr&amp;lt;T&amp;gt;::shared_ptr(const shared_ptr&amp;lt;T&amp;gt;&amp;amp; other)
    : ptr_(other.ptr_)
    , ref_cnt_(other.ref_cnt_)
{
    if (ref_cnt_)
    {
        ref_cnt_-&amp;gt;fetch_add(1, std::memory_order_relaxed);
    }
}

template &amp;lt;typename T&amp;gt;
shared_ptr&amp;lt;T&amp;gt;&amp;amp; shared_ptr&amp;lt;T&amp;gt;::operator=(std::nullptr_t)
{
    release();
    ptr_ = nullptr;
    ref_cnt_ = nullptr;
    return *this;
}

template &amp;lt;typename T&amp;gt;
shared_ptr&amp;lt;T&amp;gt;&amp;amp; shared_ptr&amp;lt;T&amp;gt;::operator=(const shared_ptr&amp;lt;T&amp;gt;&amp;amp; other)
{
    if (this != other)
    {
        release();
        ptr_ = other.ptr_;
        ref_cnt_ = other.ref_cnt_;
        if (ref_cnt_)
        {
            ref_cnt_-&amp;gt;fetch_add(1, std::memory_order_relaxed);
        }
    }
    return *this;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;调用拷贝构造时，当前对象并没有管理任何资源，无需释放，调用拷贝赋值时，需要处理当前正在管理的资源的释放操作。
引用计数的增加采用relaxed内存序，仅保证原子性，可能出现数据竞争，但不会出现撕裂现象。出现数据竞争可能导致多个线程观察到的引用计数变化顺序不一致，但这并不影响。智能指针只保证引用计数的线程安全（即当一个线程在使用资源时，另一个线程不能够释放资源），不保证资源使用的线程安全。引用计数的增加，不会导致资源释放。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;不可能出现一个线程即将释放资源，同时引用计数又增加的情况。线程即将释放资源，表示资源仅被当前线程持有，引用计数增加，只能从当前线程的对象中触发，而当前对象正在析构，是不可能被拷贝的。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;移动构造与移动赋值&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;template &amp;lt;typename T&amp;gt;
shared_ptr&amp;lt;T&amp;gt;::shared_ptr(shared_ptr&amp;lt;T&amp;gt;&amp;amp;&amp;amp; other) noexcept
    : ptr_(other.ptr_)
    , ref_cnt_(other.ref_cnt_)
{
    other.ptr_ = nullptr;
    other.ref_cnt_ = nullptr;
}

template &amp;lt;typename T&amp;gt;
shared_ptr&amp;lt;T&amp;gt;&amp;amp; shared_ptr&amp;lt;T&amp;gt;::operator=(shared_ptr&amp;lt;T&amp;gt;&amp;amp;&amp;amp; other) noexcept
{
    if (this != &amp;amp;other)
    {
        release();
        ptr_ = other.ptr_;
        ref_cnt_ = other.ref_cnt_;
        other.ptr_ = nullptr;
        other.ref_cnt_ = nullptr;
    }
    return *this;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;noexcept 确保移动操作不会抛出异常。移动时不会改变引用计数，在移动赋值时需要处理之前管理的资源的释放。&lt;/p&gt;
&lt;h2&gt;其他函数&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;template &amp;lt;typename T&amp;gt;
T&amp;amp; shared_ptr&amp;lt;T&amp;gt;::operator*() const
{
    return *ptr_;
}

template &amp;lt;typename T&amp;gt;
std::size_t shared_ptr&amp;lt;T&amp;gt;::use_count() const
{
    return ref_cnt_ ? ref_cnt_-&amp;gt;load(std::memory_order_acquire) : 0;
}

template &amp;lt;typename T&amp;gt;
bool shared_ptr&amp;lt;T&amp;gt;::unique() const
{
    return use_count() == 1;
}

template &amp;lt;typename T&amp;gt;
T* shared_ptr&amp;lt;T&amp;gt;::get() const
{
    return ptr_;
}

template &amp;lt;typename T&amp;gt;
void shared_ptr&amp;lt;T&amp;gt;::reset(T* ptr)
{
    release();
    ptr_ = ptr;
    ref_cnt_ = ptr ? new std::atomic&amp;lt;std::size_t&amp;gt;(1) : nullptr;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;读取引用计数应当使用acquire内存序，因为读取引用计数一般是为了判断后续的操作是否会影响其他线程，要避免后续操作被重排到引用计数判断之前，所以使用acquire内存序，与fetch_sub的release内存序同步。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;另一种考虑是use_count使用relaxed内存序，unique使用acquire内存序。代码如下：&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code&gt;template &amp;lt;typename T&amp;gt;
class shared_ptr
{
// ...
public:
	std::size_t use_count(std::memory_order order = std::memory_order_relaxed) const;
	bool unique() const;
// ...  
};
template &amp;lt;typename T&amp;gt;
std::size_t shared_ptr&amp;lt;T&amp;gt;::use_count(std::memory_order order) const
{
    return ref_cnt_ ? ref_cnt_-&amp;gt;load(order) : 0;
}

template &amp;lt;typename T&amp;gt;
bool shared_ptr&amp;lt;T&amp;gt;::unique() const
{
    return use_count(std::memory_order_acquire) == 1;
}
&lt;/code&gt;&lt;/pre&gt;
</content:encoded></item><item><title>操作系统学习笔记--文件系统</title><link>https://gallnut.github.io/posts/os_fs/</link><guid isPermaLink="true">https://gallnut.github.io/posts/os_fs/</guid><pubDate>Tue, 25 Feb 2025 00:00:00 GMT</pubDate><content:encoded>&lt;h1&gt;操作系统学习笔记--文件系统&lt;/h1&gt;
&lt;h2&gt;I/O系统&lt;/h2&gt;
&lt;p&gt;I/O系统由通用块层、I/O调度系统、驱动程序组成，通用块层是文件子系统与I/O子系统之间的接口，屏蔽具体的块设备之间的差异，为文件子系统提供一个统一的抽象视图。当文件子系统需要读取文件时，向通用块层中的抽象块设备发出I/O请求，通用块层将对抽象块设备的I/O请求转换为对具体块设备的I/O请求，将请求交给I/O调度系统去处理。I/O调度系统负责处理I/O请求，向块设备驱动程序发出具体的I/O命令，块设备驱动程序直接操控块设备硬件执行相应的I/O操作。&lt;/p&gt;
&lt;h2&gt;Ext文件系统&lt;/h2&gt;
&lt;p&gt;ext文件系统将FCB分为主部和次部，主部为inode,包含文件的属性信息和索引表，属性信息包含文件的类型、权限、所有者、所属组、长度、时间戳、连接数等信息，索引表表项为指向文件存储块地址的指针。FCB次部为文件名和inode编号(指针？)，文件名用于目录中的文件按名检索，inode编号用于找到文件对应的inode。&lt;/p&gt;
&lt;h2&gt;Ext3 索引结构&lt;/h2&gt;
&lt;p&gt;Ext3 的索引结构有15项，每项4B,总共60B,0～11项为直接索引指针，直接指向数据块，12～14项为间接索引指针，指向索引块，索引块中的指针指向数据块或指向下一级索引块。Ext3设立三级间接索引，可以表达大文件。假设存储块大小为1KB,12KB以内的文件只需要使用到直接索引指针，12KB～268KB大小的文件需要使用1级间接索引指针，更大的文件则需要使用二级间接索引和三级间接索引。大文件经过多级索引之后，存取效率会降低。&lt;/p&gt;
&lt;h2&gt;Ext4 索引结构&lt;/h2&gt;
&lt;p&gt;Ext4采用区段索引来描述更大的文件，文件大小可达2TB&lt;/p&gt;
</content:encoded></item></channel></rss>